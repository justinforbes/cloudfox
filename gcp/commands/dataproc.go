package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	dataprocservice "github.com/BishopFox/cloudfox/gcp/services/dataprocService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var GCPDataprocCommand = &cobra.Command{
	Use:     globals.GCP_DATAPROC_MODULE_NAME,
	Aliases: []string{"dp", "hadoop", "spark"},
	Short:   "Enumerate Dataproc clusters",
	Long: `Enumerate Dataproc (Hadoop/Spark) clusters.

Features:
- Lists all Dataproc clusters across regions
- Shows service account configuration
- Identifies public IP exposure
- Checks for Kerberos authentication
- Analyzes security configurations`,
	Run: runGCPDataprocCommand,
}

type DataprocModule struct {
	gcpinternal.BaseGCPModule
	Clusters []dataprocservice.ClusterInfo
	LootMap  map[string]*internal.LootFile
	mu       sync.Mutex
}

type DataprocOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o DataprocOutput) TableFiles() []internal.TableFile { return o.Table }
func (o DataprocOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPDataprocCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_DATAPROC_MODULE_NAME)
	if err != nil {
		return
	}

	module := &DataprocModule{
		BaseGCPModule: gcpinternal.NewBaseGCPModule(cmdCtx),
		Clusters:      []dataprocservice.ClusterInfo{},
		LootMap:       make(map[string]*internal.LootFile),
	}
	module.initializeLootFiles()
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *DataprocModule) Execute(ctx context.Context, logger internal.Logger) {
	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_DATAPROC_MODULE_NAME, m.processProject)

	if len(m.Clusters) == 0 {
		logger.InfoM("No Dataproc clusters found", globals.GCP_DATAPROC_MODULE_NAME)
		return
	}

	runningCount := 0
	publicCount := 0
	for _, cluster := range m.Clusters {
		if cluster.State == "RUNNING" {
			runningCount++
		}
		if !cluster.InternalIPOnly {
			publicCount++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d Dataproc cluster(s) (%d running, %d with public IPs)",
		len(m.Clusters), runningCount, publicCount), globals.GCP_DATAPROC_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *DataprocModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Enumerating Dataproc in project: %s", projectID), globals.GCP_DATAPROC_MODULE_NAME)
	}

	svc := dataprocservice.New()

	clusters, err := svc.ListClusters(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_DATAPROC_MODULE_NAME,
			fmt.Sprintf("Could not list Dataproc clusters in project %s", projectID))
		return
	}

	m.mu.Lock()
	m.Clusters = append(m.Clusters, clusters...)
	for _, cluster := range clusters {
		m.addToLoot(cluster)
	}
	m.mu.Unlock()
}

func (m *DataprocModule) initializeLootFiles() {
	m.LootMap["dataproc-clusters"] = &internal.LootFile{
		Name:     "dataproc-clusters",
		Contents: "# Dataproc Clusters\n# Generated by CloudFox\n\n",
	}
	m.LootMap["dataproc-service-accounts"] = &internal.LootFile{
		Name:     "dataproc-service-accounts",
		Contents: "",
	}
	m.LootMap["dataproc-buckets"] = &internal.LootFile{
		Name:     "dataproc-buckets",
		Contents: "",
	}
}

func (m *DataprocModule) addToLoot(cluster dataprocservice.ClusterInfo) {
	m.LootMap["dataproc-clusters"].Contents += fmt.Sprintf(
		"# Cluster: %s\n# Region: %s\n# State: %s\n# Service Account: %s\n# Public IPs: %v\n\n",
		cluster.Name, cluster.Region, cluster.State, cluster.ServiceAccount, !cluster.InternalIPOnly)

	if cluster.ServiceAccount != "" {
		m.LootMap["dataproc-service-accounts"].Contents += cluster.ServiceAccount + "\n"
	}

	if cluster.ConfigBucket != "" {
		m.LootMap["dataproc-buckets"].Contents += fmt.Sprintf("gs://%s # config bucket for %s\n", cluster.ConfigBucket, cluster.Name)
	}
	if cluster.TempBucket != "" {
		m.LootMap["dataproc-buckets"].Contents += fmt.Sprintf("gs://%s # temp bucket for %s\n", cluster.TempBucket, cluster.Name)
	}
}

func (m *DataprocModule) writeOutput(ctx context.Context, logger internal.Logger) {
	var tables []internal.TableFile

	// Clusters table
	header := []string{"Name", "Region", "State", "Master", "Workers", "Service Account", "Public IPs", "Kerberos", "Risk", "Project Name", "Project"}
	var body [][]string
	for _, cluster := range m.Clusters {
		publicIPs := "No"
		if !cluster.InternalIPOnly {
			publicIPs = "Yes"
		}
		kerberos := "No"
		if cluster.KerberosEnabled {
			kerberos = "Yes"
		}
		sa := cluster.ServiceAccount
		if sa == "" {
			sa = "(default)"
		} else if len(sa) > 35 {
			sa = sa[:32] + "..."
		}
		masterConfig := fmt.Sprintf("%s x%d", cluster.MasterMachineType, cluster.MasterCount)
		workerConfig := fmt.Sprintf("%s x%d", cluster.WorkerMachineType, cluster.WorkerCount)

		body = append(body, []string{
			cluster.Name,
			cluster.Region,
			cluster.State,
			masterConfig,
			workerConfig,
			sa,
			publicIPs,
			kerberos,
			cluster.RiskLevel,
			m.GetProjectName(cluster.ProjectID),
			cluster.ProjectID,
		})
	}
	tables = append(tables, internal.TableFile{
		Name:   "dataproc-clusters",
		Header: header,
		Body:   body,
	})

	// High-risk findings
	var highRiskBody [][]string
	for _, cluster := range m.Clusters {
		if cluster.RiskLevel == "HIGH" || cluster.RiskLevel == "MEDIUM" {
			highRiskBody = append(highRiskBody, []string{
				cluster.Name,
				cluster.RiskLevel,
				strings.Join(cluster.RiskReasons, "; "),
				m.GetProjectName(cluster.ProjectID),
				cluster.ProjectID,
			})
		}
	}

	if len(highRiskBody) > 0 {
		tables = append(tables, internal.TableFile{
			Name:   "dataproc-risks",
			Header: []string{"Cluster", "Risk Level", "Reasons", "Project Name", "Project"},
			Body:   highRiskBody,
		})
	}

	var lootFiles []internal.LootFile
	for _, loot := range m.LootMap {
		if loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n\n") {
			lootFiles = append(lootFiles, *loot)
		}
	}

	output := DataprocOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart("gcp", m.Format, m.OutputDirectory, m.Verbosity, m.WrapTable,
		"project", m.ProjectIDs, scopeNames, m.Account, output)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_DATAPROC_MODULE_NAME)
	}
}
