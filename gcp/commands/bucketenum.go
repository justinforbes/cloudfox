package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	bucketenumservice "github.com/BishopFox/cloudfox/gcp/services/bucketEnumService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var (
	bucketEnumMaxObjects int
)

var GCPBucketEnumCommand = &cobra.Command{
	Use:     globals.GCP_BUCKETENUM_MODULE_NAME,
	Aliases: []string{"bucket-scan", "gcs-enum", "sensitive-files"},
	Short:   "Enumerate GCS buckets for sensitive files (credentials, secrets, configs)",
	Long: `Enumerate GCS buckets to find potentially sensitive files.

This module scans bucket contents for files that may contain:
- Credentials (service account keys, SSH keys, certificates)
- Secrets (environment files, API keys, tokens)
- Configuration files (may contain hardcoded secrets)
- Database backups
- Terraform state files
- Source code/git repositories

File categories detected:
- Credential: .json keys, .pem, .key, .p12, SSH keys
- Secret: .env, passwords, API keys, tokens
- Config: YAML, properties, settings files
- Backup: SQL dumps, archives
- Source: Git repositories
- Cloud: Cloud Functions source, build artifacts

WARNING: This may take a long time for buckets with many objects.
Use --max-objects to limit the scan.`,
	Run: runGCPBucketEnumCommand,
}

func init() {
	GCPBucketEnumCommand.Flags().IntVar(&bucketEnumMaxObjects, "max-objects", 1000, "Maximum objects to scan per bucket (0 for unlimited)")
}

type BucketEnumModule struct {
	gcpinternal.BaseGCPModule
	ProjectSensitiveFiles map[string][]bucketenumservice.SensitiveFileInfo // projectID -> files
	LootMap               map[string]map[string]*internal.LootFile         // projectID -> loot files
	mu                    sync.Mutex
}

type BucketEnumOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o BucketEnumOutput) TableFiles() []internal.TableFile { return o.Table }
func (o BucketEnumOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPBucketEnumCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_BUCKETENUM_MODULE_NAME)
	if err != nil {
		return
	}

	module := &BucketEnumModule{
		BaseGCPModule:         gcpinternal.NewBaseGCPModule(cmdCtx),
		ProjectSensitiveFiles: make(map[string][]bucketenumservice.SensitiveFileInfo),
		LootMap:               make(map[string]map[string]*internal.LootFile),
	}
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *BucketEnumModule) Execute(ctx context.Context, logger internal.Logger) {
	logger.InfoM(fmt.Sprintf("Scanning buckets for sensitive files (max %d objects per bucket)...", bucketEnumMaxObjects), globals.GCP_BUCKETENUM_MODULE_NAME)
	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_BUCKETENUM_MODULE_NAME, m.processProject)

	allFiles := m.getAllSensitiveFiles()
	if len(allFiles) == 0 {
		logger.InfoM("No sensitive files found", globals.GCP_BUCKETENUM_MODULE_NAME)
		return
	}

	// Count by risk level
	criticalCount := 0
	highCount := 0
	for _, file := range allFiles {
		switch file.RiskLevel {
		case "CRITICAL":
			criticalCount++
		case "HIGH":
			highCount++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d potentially sensitive file(s) (%d CRITICAL, %d HIGH)",
		len(allFiles), criticalCount, highCount), globals.GCP_BUCKETENUM_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *BucketEnumModule) getAllSensitiveFiles() []bucketenumservice.SensitiveFileInfo {
	var all []bucketenumservice.SensitiveFileInfo
	for _, files := range m.ProjectSensitiveFiles {
		all = append(all, files...)
	}
	return all
}

func (m *BucketEnumModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Scanning buckets in project: %s", projectID), globals.GCP_BUCKETENUM_MODULE_NAME)
	}

	svc := bucketenumservice.New()

	m.mu.Lock()
	// Initialize loot for this project
	if m.LootMap[projectID] == nil {
		m.LootMap[projectID] = make(map[string]*internal.LootFile)
		m.LootMap[projectID]["bucket-enum-sensitive-commands"] = &internal.LootFile{
			Name:     "bucket-enum-sensitive-commands",
			Contents: "# GCS Download Commands for CRITICAL/HIGH Risk Files\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
		}
		m.LootMap[projectID]["bucket-enum-commands"] = &internal.LootFile{
			Name:     "bucket-enum-commands",
			Contents: "# GCS Download Commands for All Detected Files\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
		}
	}
	m.mu.Unlock()

	// Get list of buckets
	buckets, err := svc.GetBucketsList(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_BUCKETENUM_MODULE_NAME,
			fmt.Sprintf("Could not enumerate buckets in project %s", projectID))
		return
	}

	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Found %d bucket(s) in project %s", len(buckets), projectID), globals.GCP_BUCKETENUM_MODULE_NAME)
	}

	// Scan each bucket
	var projectFiles []bucketenumservice.SensitiveFileInfo
	for _, bucketName := range buckets {
		files, err := svc.EnumerateBucketSensitiveFiles(bucketName, projectID, bucketEnumMaxObjects)
		if err != nil {
			m.CommandCounter.Error++
			gcpinternal.HandleGCPError(err, logger, globals.GCP_BUCKETENUM_MODULE_NAME,
				fmt.Sprintf("Could not scan bucket %s in project %s", bucketName, projectID))
			continue
		}
		projectFiles = append(projectFiles, files...)
	}

	m.mu.Lock()
	m.ProjectSensitiveFiles[projectID] = projectFiles
	for _, file := range projectFiles {
		m.addFileToLoot(projectID, file)
	}
	m.mu.Unlock()
}

func (m *BucketEnumModule) addFileToLoot(projectID string, file bucketenumservice.SensitiveFileInfo) {
	// All files go to the general commands file
	if lootFile := m.LootMap[projectID]["bucket-enum-commands"]; lootFile != nil {
		lootFile.Contents += fmt.Sprintf(
			"# [%s] %s - gs://%s/%s\n"+
				"# Category: %s, Size: %d bytes\n"+
				"%s\n\n",
			file.RiskLevel, file.Category,
			file.BucketName, file.ObjectName,
			file.Description, file.Size,
			file.DownloadCmd,
		)
	}

	// CRITICAL and HIGH risk files also go to the sensitive commands file
	if file.RiskLevel == "CRITICAL" || file.RiskLevel == "HIGH" {
		if lootFile := m.LootMap[projectID]["bucket-enum-sensitive-commands"]; lootFile != nil {
			lootFile.Contents += fmt.Sprintf(
				"# [%s] %s - gs://%s/%s\n"+
					"# Category: %s, Size: %d bytes\n"+
					"%s\n\n",
				file.RiskLevel, file.Category,
				file.BucketName, file.ObjectName,
				file.Description, file.Size,
				file.DownloadCmd,
			)
		}
	}
}

func (m *BucketEnumModule) writeOutput(ctx context.Context, logger internal.Logger) {
	if m.Hierarchy != nil && !m.FlatOutput {
		m.writeHierarchicalOutput(ctx, logger)
	} else {
		m.writeFlatOutput(ctx, logger)
	}
}

func (m *BucketEnumModule) getFilesHeader() []string {
	return []string{"Project ID", "Project Name", "Bucket", "Object Name", "Category", "Size", "Public", "Description"}
}

func (m *BucketEnumModule) getSensitiveFilesHeader() []string {
	return []string{"Project ID", "Project Name", "Bucket", "Object Name", "Category", "Size", "Public"}
}

func (m *BucketEnumModule) filesToTableBody(files []bucketenumservice.SensitiveFileInfo) [][]string {
	var body [][]string
	for _, file := range files {
		publicStatus := "No"
		if file.IsPublic {
			publicStatus = "Yes"
		}
		body = append(body, []string{
			file.ProjectID,
			m.GetProjectName(file.ProjectID),
			file.BucketName,
			file.ObjectName,
			file.Category,
			formatFileSize(file.Size),
			publicStatus,
			file.Description,
		})
	}
	return body
}

func (m *BucketEnumModule) sensitiveFilesToTableBody(files []bucketenumservice.SensitiveFileInfo) [][]string {
	var body [][]string
	for _, file := range files {
		if file.RiskLevel == "CRITICAL" || file.RiskLevel == "HIGH" {
			publicStatus := "No"
			if file.IsPublic {
				publicStatus = "Yes"
			}
			body = append(body, []string{
				file.ProjectID,
				m.GetProjectName(file.ProjectID),
				file.BucketName,
				file.ObjectName,
				file.Category,
				formatFileSize(file.Size),
				publicStatus,
			})
		}
	}
	return body
}

func (m *BucketEnumModule) buildTablesForProject(projectID string) []internal.TableFile {
	var tableFiles []internal.TableFile

	files := m.ProjectSensitiveFiles[projectID]
	if len(files) > 0 {
		tableFiles = append(tableFiles, internal.TableFile{
			Name:   "bucket-enum",
			Header: m.getFilesHeader(),
			Body:   m.filesToTableBody(files),
		})

		sensitiveBody := m.sensitiveFilesToTableBody(files)
		if len(sensitiveBody) > 0 {
			tableFiles = append(tableFiles, internal.TableFile{
				Name:   "bucket-enum-sensitive",
				Header: m.getSensitiveFilesHeader(),
				Body:   sensitiveBody,
			})
		}
	}

	return tableFiles
}

func (m *BucketEnumModule) writeHierarchicalOutput(ctx context.Context, logger internal.Logger) {
	outputData := internal.HierarchicalOutputData{
		OrgLevelData:     make(map[string]internal.CloudfoxOutput),
		ProjectLevelData: make(map[string]internal.CloudfoxOutput),
	}

	for projectID := range m.ProjectSensitiveFiles {
		tableFiles := m.buildTablesForProject(projectID)

		var lootFiles []internal.LootFile
		if projectLoot, ok := m.LootMap[projectID]; ok {
			for _, loot := range projectLoot {
				if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# WARNING: Only use with proper authorization\n\n") {
					lootFiles = append(lootFiles, *loot)
				}
			}
		}

		outputData.ProjectLevelData[projectID] = BucketEnumOutput{Table: tableFiles, Loot: lootFiles}
	}

	pathBuilder := m.BuildPathBuilder()

	err := internal.HandleHierarchicalOutputSmart("gcp", m.Format, m.Verbosity, m.WrapTable, pathBuilder, outputData)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing hierarchical output: %v", err), globals.GCP_BUCKETENUM_MODULE_NAME)
	}
}

func (m *BucketEnumModule) writeFlatOutput(ctx context.Context, logger internal.Logger) {
	allFiles := m.getAllSensitiveFiles()

	var tables []internal.TableFile

	if len(allFiles) > 0 {
		tables = append(tables, internal.TableFile{
			Name:   "bucket-enum",
			Header: m.getFilesHeader(),
			Body:   m.filesToTableBody(allFiles),
		})

		sensitiveBody := m.sensitiveFilesToTableBody(allFiles)
		if len(sensitiveBody) > 0 {
			tables = append(tables, internal.TableFile{
				Name:   "bucket-enum-sensitive",
				Header: m.getSensitiveFilesHeader(),
				Body:   sensitiveBody,
			})
			logger.InfoM(fmt.Sprintf("[FINDING] Found %d CRITICAL/HIGH risk files!", len(sensitiveBody)), globals.GCP_BUCKETENUM_MODULE_NAME)
		}
	}

	var lootFiles []internal.LootFile
	for _, projectLoot := range m.LootMap {
		for _, loot := range projectLoot {
			if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# WARNING: Only use with proper authorization\n\n") {
				lootFiles = append(lootFiles, *loot)
			}
		}
	}

	output := BucketEnumOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart("gcp", m.Format, m.OutputDirectory, m.Verbosity, m.WrapTable,
		"project", m.ProjectIDs, scopeNames, m.Account, output)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_BUCKETENUM_MODULE_NAME)
	}
}

func formatFileSize(bytes int64) string {
	const (
		KB = 1024
		MB = KB * 1024
		GB = MB * 1024
	)

	switch {
	case bytes >= GB:
		return fmt.Sprintf("%.1f GB", float64(bytes)/GB)
	case bytes >= MB:
		return fmt.Sprintf("%.1f MB", float64(bytes)/MB)
	case bytes >= KB:
		return fmt.Sprintf("%.1f KB", float64(bytes)/KB)
	default:
		return fmt.Sprintf("%d B", bytes)
	}
}
