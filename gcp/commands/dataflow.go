package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	dataflowservice "github.com/BishopFox/cloudfox/gcp/services/dataflowService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var GCPDataflowCommand = &cobra.Command{
	Use:     globals.GCP_DATAFLOW_MODULE_NAME,
	Aliases: []string{"df", "pipelines"},
	Short:   "Enumerate Dataflow jobs and pipelines",
	Long: `Enumerate Dataflow jobs with security analysis.

Features:
- Lists all Dataflow jobs (batch and streaming)
- Shows service account configuration
- Identifies network exposure (public IPs)
- Analyzes temp/staging storage locations
- Detects default service account usage`,
	Run: runGCPDataflowCommand,
}

type DataflowModule struct {
	gcpinternal.BaseGCPModule
	ProjectJobs  map[string][]dataflowservice.JobInfo    // projectID -> jobs
	LootMap      map[string]map[string]*internal.LootFile // projectID -> loot files
	FoxMapperCache *gcpinternal.FoxMapperCache           // Cached FoxMapper analysis results
	mu           sync.Mutex
}

type DataflowOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o DataflowOutput) TableFiles() []internal.TableFile { return o.Table }
func (o DataflowOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPDataflowCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_DATAFLOW_MODULE_NAME)
	if err != nil {
		return
	}

	module := &DataflowModule{
		BaseGCPModule: gcpinternal.NewBaseGCPModule(cmdCtx),
		ProjectJobs:   make(map[string][]dataflowservice.JobInfo),
		LootMap:       make(map[string]map[string]*internal.LootFile),
	}
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *DataflowModule) Execute(ctx context.Context, logger internal.Logger) {
	// Get FoxMapper cache from context
	m.FoxMapperCache = gcpinternal.GetFoxMapperCacheFromContext(ctx)

	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_DATAFLOW_MODULE_NAME, m.processProject)

	allJobs := m.getAllJobs()
	if len(allJobs) == 0 {
		logger.InfoM("No Dataflow jobs found", globals.GCP_DATAFLOW_MODULE_NAME)
		return
	}

	// Count by state
	running := 0
	publicIPs := 0
	for _, job := range allJobs {
		if job.State == "JOB_STATE_RUNNING" {
			running++
		}
		if job.UsePublicIPs {
			publicIPs++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d Dataflow job(s) (%d running, %d with public IPs)",
		len(allJobs), running, publicIPs), globals.GCP_DATAFLOW_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *DataflowModule) getAllJobs() []dataflowservice.JobInfo {
	var all []dataflowservice.JobInfo
	for _, jobs := range m.ProjectJobs {
		all = append(all, jobs...)
	}
	return all
}

func (m *DataflowModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Enumerating Dataflow in project: %s", projectID), globals.GCP_DATAFLOW_MODULE_NAME)
	}

	svc := dataflowservice.New()
	jobs, err := svc.ListJobs(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_DATAFLOW_MODULE_NAME,
			fmt.Sprintf("Could not list Dataflow jobs in project %s", projectID))
		return
	}

	m.mu.Lock()
	m.ProjectJobs[projectID] = jobs

	// Initialize loot for this project
	if m.LootMap[projectID] == nil {
		m.LootMap[projectID] = make(map[string]*internal.LootFile)
		m.LootMap[projectID]["dataflow-commands"] = &internal.LootFile{
			Name:     "dataflow-commands",
			Contents: "# Dataflow Commands\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
		}
	}

	for _, job := range jobs {
		m.addToLoot(projectID, job)
	}
	m.mu.Unlock()
}

func (m *DataflowModule) addToLoot(projectID string, job dataflowservice.JobInfo) {
	lootFile := m.LootMap[projectID]["dataflow-commands"]
	if lootFile == nil {
		return
	}
	lootFile.Contents += fmt.Sprintf(
		"# =============================================================================\n"+
			"# DATAFLOW JOB: %s\n"+
			"# =============================================================================\n"+
			"# Project: %s\n"+
			"# Location: %s\n"+
			"# ID: %s\n"+
			"# Type: %s\n"+
			"# State: %s\n"+
			"# Service Account: %s\n"+
			"# Public IPs: %v\n"+
			"# Workers: %d\n",
		job.Name, job.ProjectID, job.Location,
		job.ID, job.Type, job.State,
		job.ServiceAccount, job.UsePublicIPs, job.NumWorkers,
	)

	lootFile.Contents += fmt.Sprintf(`
# === ENUMERATION COMMANDS ===

# Describe job:
gcloud dataflow jobs describe %s --project=%s --region=%s

# Show job details:
gcloud dataflow jobs show %s --project=%s --region=%s

# List all Dataflow jobs:
gcloud dataflow jobs list --project=%s --region=%s

# Get job metrics:
gcloud dataflow metrics list %s --project=%s --region=%s

`,
		job.ID, job.ProjectID, job.Location,
		job.ID, job.ProjectID, job.Location,
		job.ProjectID, job.Location,
		job.ID, job.ProjectID, job.Location,
	)

	// Bucket inspection
	if job.TempLocation != "" {
		lootFile.Contents += fmt.Sprintf(
			"# Inspect temp bucket (may contain intermediate data):\n"+
				"gsutil ls -r %s\n\n",
			job.TempLocation,
		)
	}
	if job.StagingLocation != "" {
		lootFile.Contents += fmt.Sprintf(
			"# Inspect staging bucket (contains job artifacts):\n"+
				"gsutil ls -r %s\n\n",
			job.StagingLocation,
		)
	}

	// === EXPLOIT COMMANDS ===
	lootFile.Contents += "# === EXPLOIT COMMANDS ===\n\n"

	lootFile.Contents += fmt.Sprintf(
		"# Cancel running job:\n"+
			"gcloud dataflow jobs cancel %s --project=%s --region=%s\n\n"+
			"# Drain running job (graceful stop):\n"+
			"gcloud dataflow jobs drain %s --project=%s --region=%s\n\n"+
			"# Submit a new Dataflow job (code execution as SA: %s):\n"+
			"# Template-based job:\n"+
			"gcloud dataflow jobs run cloudfox-test --gcs-location=gs://dataflow-templates/latest/Word_Count --region=%s --project=%s --parameters=inputFile=gs://BUCKET/input.txt,output=gs://BUCKET/output\n\n"+
			"# Flex template job (custom container = full code execution):\n"+
			"gcloud dataflow flex-template run cloudfox-flex --template-file-gcs-location=gs://YOUR_BUCKET/template.json --region=%s --project=%s --service-account-email=%s\n\n",
		job.ID, job.ProjectID, job.Location,
		job.ID, job.ProjectID, job.Location,
		job.ServiceAccount,
		job.Location, job.ProjectID,
		job.Location, job.ProjectID, job.ServiceAccount,
	)

	// Inspect staging/temp for secrets
	if job.TempLocation != "" || job.StagingLocation != "" {
		lootFile.Contents += "# Search job buckets for secrets/credentials:\n"
		if job.TempLocation != "" {
			lootFile.Contents += fmt.Sprintf("gsutil cat %s/** 2>/dev/null | grep -iE '(password|secret|token|key|credential)'\n", job.TempLocation)
		}
		if job.StagingLocation != "" {
			lootFile.Contents += fmt.Sprintf("gsutil cat %s/** 2>/dev/null | grep -iE '(password|secret|token|key|credential)'\n", job.StagingLocation)
		}
		lootFile.Contents += "\n"
	}
}

func (m *DataflowModule) writeOutput(ctx context.Context, logger internal.Logger) {
	if m.Hierarchy != nil && !m.FlatOutput {
		m.writeHierarchicalOutput(ctx, logger)
	} else {
		m.writeFlatOutput(ctx, logger)
	}
}

func (m *DataflowModule) getTableHeader() []string {
	return []string{
		"Project ID",
		"Project Name",
		"Name",
		"Type",
		"State",
		"Location",
		"Service Account",
		"SA Attack Paths",
		"Public IPs",
		"Workers",
	}
}

func (m *DataflowModule) jobsToTableBody(jobs []dataflowservice.JobInfo) [][]string {
	var body [][]string
	for _, job := range jobs {
		publicIPs := "No"
		if job.UsePublicIPs {
			publicIPs = "Yes"
		}

		// Check attack paths (privesc/exfil/lateral) for the service account
		attackPaths := "run foxmapper"
		if m.FoxMapperCache != nil && m.FoxMapperCache.IsPopulated() {
			if job.ServiceAccount != "" {
				attackPaths = gcpinternal.GetAttackSummaryFromCaches(m.FoxMapperCache, nil, job.ServiceAccount)
			} else {
				attackPaths = "No"
			}
		}

		body = append(body, []string{
			job.ProjectID,
			m.GetProjectName(job.ProjectID),
			job.Name,
			job.Type,
			job.State,
			job.Location,
			job.ServiceAccount,
			attackPaths,
			publicIPs,
			fmt.Sprintf("%d", job.NumWorkers),
		})
	}
	return body
}

func (m *DataflowModule) writeHierarchicalOutput(ctx context.Context, logger internal.Logger) {
	outputData := internal.HierarchicalOutputData{
		OrgLevelData:     make(map[string]internal.CloudfoxOutput),
		ProjectLevelData: make(map[string]internal.CloudfoxOutput),
	}

	for projectID, jobs := range m.ProjectJobs {
		body := m.jobsToTableBody(jobs)
		tableFiles := []internal.TableFile{{Name: "dataflow", Header: m.getTableHeader(), Body: body}}

		var lootFiles []internal.LootFile
		if projectLoot, ok := m.LootMap[projectID]; ok {
			for _, loot := range projectLoot {
				if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n") {
					lootFiles = append(lootFiles, *loot)
				}
			}
		}

		outputData.ProjectLevelData[projectID] = DataflowOutput{Table: tableFiles, Loot: lootFiles}
	}

	pathBuilder := m.BuildPathBuilder()

	err := internal.HandleHierarchicalOutputSmart("gcp", m.Format, m.Verbosity, m.WrapTable, pathBuilder, outputData)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing hierarchical output: %v", err), globals.GCP_DATAFLOW_MODULE_NAME)
	}
}

func (m *DataflowModule) writeFlatOutput(ctx context.Context, logger internal.Logger) {
	allJobs := m.getAllJobs()
	body := m.jobsToTableBody(allJobs)

	var lootFiles []internal.LootFile
	for _, projectLoot := range m.LootMap {
		for _, loot := range projectLoot {
			if loot != nil && loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n") {
				lootFiles = append(lootFiles, *loot)
			}
		}
	}

	tables := []internal.TableFile{{Name: "dataflow", Header: m.getTableHeader(), Body: body}}
	output := DataflowOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart("gcp", m.Format, m.OutputDirectory, m.Verbosity, m.WrapTable,
		"project", m.ProjectIDs, scopeNames, m.Account, output)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_DATAFLOW_MODULE_NAME)
	}
}
