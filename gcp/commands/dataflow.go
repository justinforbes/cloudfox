package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	dataflowservice "github.com/BishopFox/cloudfox/gcp/services/dataflowService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var GCPDataflowCommand = &cobra.Command{
	Use:     globals.GCP_DATAFLOW_MODULE_NAME,
	Aliases: []string{"df", "pipelines"},
	Short:   "Enumerate Dataflow jobs and pipelines",
	Long: `Enumerate Dataflow jobs with security analysis.

Features:
- Lists all Dataflow jobs (batch and streaming)
- Shows service account configuration
- Identifies network exposure (public IPs)
- Analyzes temp/staging storage locations
- Detects default service account usage`,
	Run: runGCPDataflowCommand,
}

type DataflowModule struct {
	gcpinternal.BaseGCPModule
	Jobs    []dataflowservice.JobInfo
	LootMap map[string]*internal.LootFile
	mu      sync.Mutex
}

type DataflowOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o DataflowOutput) TableFiles() []internal.TableFile { return o.Table }
func (o DataflowOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPDataflowCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_DATAFLOW_MODULE_NAME)
	if err != nil {
		return
	}

	module := &DataflowModule{
		BaseGCPModule: gcpinternal.NewBaseGCPModule(cmdCtx),
		Jobs:          []dataflowservice.JobInfo{},
		LootMap:       make(map[string]*internal.LootFile),
	}
	module.initializeLootFiles()
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *DataflowModule) Execute(ctx context.Context, logger internal.Logger) {
	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_DATAFLOW_MODULE_NAME, m.processProject)

	if len(m.Jobs) == 0 {
		logger.InfoM("No Dataflow jobs found", globals.GCP_DATAFLOW_MODULE_NAME)
		return
	}

	// Count by state
	running := 0
	publicIPs := 0
	for _, job := range m.Jobs {
		if job.State == "JOB_STATE_RUNNING" {
			running++
		}
		if job.UsePublicIPs {
			publicIPs++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d Dataflow job(s) (%d running, %d with public IPs)",
		len(m.Jobs), running, publicIPs), globals.GCP_DATAFLOW_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *DataflowModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Enumerating Dataflow in project: %s", projectID), globals.GCP_DATAFLOW_MODULE_NAME)
	}

	svc := dataflowservice.New()
	jobs, err := svc.ListJobs(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_DATAFLOW_MODULE_NAME,
			fmt.Sprintf("Could not list Dataflow jobs in project %s", projectID))
		return
	}

	m.mu.Lock()
	m.Jobs = append(m.Jobs, jobs...)
	for _, job := range jobs {
		m.addToLoot(job)
	}
	m.mu.Unlock()
}

func (m *DataflowModule) initializeLootFiles() {
	m.LootMap["dataflow-jobs"] = &internal.LootFile{
		Name:     "dataflow-jobs",
		Contents: "# Dataflow Jobs\n# Generated by CloudFox\n\n",
	}
	m.LootMap["dataflow-service-accounts"] = &internal.LootFile{
		Name:     "dataflow-service-accounts",
		Contents: "",
	}
}

func (m *DataflowModule) addToLoot(job dataflowservice.JobInfo) {
	m.LootMap["dataflow-jobs"].Contents += fmt.Sprintf(
		"# Job: %s (%s)\n# Type: %s\n# State: %s\n# Service Account: %s\n# Public IPs: %v\n\n",
		job.Name, job.ID, job.Type, job.State, job.ServiceAccount, job.UsePublicIPs)

	if job.ServiceAccount != "" {
		m.LootMap["dataflow-service-accounts"].Contents += job.ServiceAccount + "\n"
	}
}

func (m *DataflowModule) writeOutput(ctx context.Context, logger internal.Logger) {
	header := []string{
		"Name", "Type", "State", "Location", "Service Account",
		"Public IPs", "Workers", "Risk", "Project Name", "Project",
	}

	var body [][]string
	for _, job := range m.Jobs {
		publicIPs := "No"
		if job.UsePublicIPs {
			publicIPs = "Yes"
		}

		sa := job.ServiceAccount
		if sa == "" {
			sa = "(default)"
		} else if len(sa) > 40 {
			sa = sa[:37] + "..."
		}

		body = append(body, []string{
			job.Name,
			job.Type,
			job.State,
			job.Location,
			sa,
			publicIPs,
			fmt.Sprintf("%d", job.NumWorkers),
			job.RiskLevel,
			m.GetProjectName(job.ProjectID),
			job.ProjectID,
		})
	}

	var lootFiles []internal.LootFile
	for _, loot := range m.LootMap {
		if loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n\n") {
			lootFiles = append(lootFiles, *loot)
		}
	}

	tables := []internal.TableFile{{Name: "dataflow", Header: header, Body: body}}

	// High-risk jobs table
	var highRiskBody [][]string
	for _, job := range m.Jobs {
		if job.RiskLevel == "HIGH" || job.RiskLevel == "MEDIUM" {
			highRiskBody = append(highRiskBody, []string{
				job.Name,
				job.RiskLevel,
				strings.Join(job.RiskReasons, "; "),
				m.GetProjectName(job.ProjectID),
				job.ProjectID,
			})
		}
	}

	if len(highRiskBody) > 0 {
		tables = append(tables, internal.TableFile{
			Name:   "dataflow-risks",
			Header: []string{"Job", "Risk Level", "Reasons", "Project Name", "Project"},
			Body:   highRiskBody,
		})
	}

	output := DataflowOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart("gcp", m.Format, m.OutputDirectory, m.Verbosity, m.WrapTable,
		"project", m.ProjectIDs, scopeNames, m.Account, output)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_DATAFLOW_MODULE_NAME)
	}
}
